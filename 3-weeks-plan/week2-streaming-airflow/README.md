### ğŸ“… Week 2 â€“ Streaming + Orchestration
 - Project 1: Real-Time Market Data Stream (Simulated)
   - Tech Stack: Python, Apache Kafka
 - Project 2: Airflow DAG for Batch ETL
   - Tech Stack: Apache Airflow

### ğŸ”¹ Description
- Kafka producer generates random market data every second.
- Kafka consumer reads, parses, and writes to a local DB or file.
- Airflow DAG schedules the Week 1 batch pipeline with retries and alerts.
```
ğŸ“ Structure
week2-streaming-airflow/
â”œâ”€â”€ kafka/
â”‚ â”œâ”€â”€ producer.py
â”‚ â””â”€â”€ consumer.py
â”œâ”€â”€ airflow/
â”‚ â””â”€â”€ dags/
â”‚ â””â”€â”€ covid_etl_dag.py
â””â”€â”€ README.md
```

### ğŸš€ How to Run Kafka (local)
```
docker-compose up -d # start Kafka
python kafka/producer.py
python kafka/consumer.py
```
### ğŸš€ How to Run Airflow
```
cd airflow
docker-compose up
```
