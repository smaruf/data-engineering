# ğŸ§  Data Engineering Mini Projects

Welcome! This repository contains a series of data engineering projects built over 3 weeks, focusing on batch ETL, real-time streaming, orchestration, and cloud data workflows.

---

## ğŸ“… Week 1 â€“ Batch ETL Pipeline

**Project:** COVID-19 Data ETL Pipeline  
**Tech Stack:** Python, Pandas, PostgreSQL, SQLAlchemy  

### ğŸ”¹ Description  
This project extracts COVID-19 statistics from a public API, transforms the data with pandas, and loads it into a PostgreSQL database.  
Airflow DAG added in Week 2 for scheduling.

### ğŸ“ Structure  
week1-batch-etl/  
â”œâ”€â”€ etl_pipeline.py  
â”œâ”€â”€ requirements.txt  
â”œâ”€â”€ config.ini  
â””â”€â”€ README.md  

### ğŸš€ How to Run  
cd week1-batch-etl  
pip install -r requirements.txt  
python etl_pipeline.py  

---

## ğŸ“… Week 2 â€“ Streaming + Orchestration

**Project 1:** Real-Time Market Data Stream (Simulated)  
**Tech Stack:** Python, Apache Kafka  

**Project 2:** Airflow DAG for Batch ETL  
**Tech Stack:** Apache Airflow  

### ğŸ”¹ Description  
- Kafka producer generates random market data every second.  
- Kafka consumer reads, parses, and writes to a local DB or file.  
- Airflow DAG schedules the Week 1 batch pipeline with retries and alerts.

### ğŸ“ Structure  
week2-streaming-airflow/  
â”œâ”€â”€ kafka/  
â”‚   â”œâ”€â”€ producer.py  
â”‚   â””â”€â”€ consumer.py  
â”œâ”€â”€ airflow/  
â”‚   â””â”€â”€ dags/  
â”‚       â””â”€â”€ covid_etl_dag.py  
â””â”€â”€ README.md  

### ğŸš€ How to Run Kafka (local)  
docker-compose up -d  # start Kafka  
python kafka/producer.py  
python kafka/consumer.py  

### ğŸš€ How to Run Airflow  
cd airflow  
docker-compose up  

---

## ğŸ“… Week 3 â€“ Cloud Data Pipeline (AWS)

**Project:** Serverless Data Pipeline on AWS  
**Tech Stack:** AWS S3, AWS Glue, PySpark, Redshift (optional)

### ğŸ”¹ Description  
- Uploads raw CSV data to S3.  
- AWS Glue ETL job cleans and transforms data using PySpark.  
- Outputs to partitioned S3 location or Redshift.

### ğŸ“ Structure  
week3-cloud-etl/  
â”œâ”€â”€ glue_job/  
â”‚   â””â”€â”€ covid_transform.py  
â”œâ”€â”€ terraform/  
â”‚   â””â”€â”€ s3_redshift_setup.tf  
â””â”€â”€ README.md  

### ğŸš€ How to Deploy  
```bash
cd week3-cloud-etl/terraform
terraform init && terraform apply
```

### ğŸŒ AWS Setup  
- S3 buckets for raw and processed data  
- AWS Glue job with PySpark transformation  
- IAM roles and policies  
- (Optional) Redshift cluster via Terraform  

---

## ğŸ”— Useful Links  
- DataTalksClub DE Zoomcamp: https://github.com/DataTalksClub/data-engineering-zoomcamp  
- Airflow Docs: https://airflow.apache.org/docs/  
- Confluent Kafka Python: https://developer.confluent.io/get-started/python/  
- AWS Glue Tutorial: https://docs.aws.amazon.com/glue/latest/dg/start-working.html  

---

## ğŸ§” About Me  
**Muhammad Shamsul Maruf**  
Backend & Data Engineer | Java, Python, Kafka, AWS  
LinkedIn: https://www.linkedin.com/in/muhammad-shamsul-maruf-79905161/  
GitHub: https://github.com/smaruf  
